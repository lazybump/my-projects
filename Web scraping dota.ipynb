{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc11e42",
   "metadata": {},
   "source": [
    "# Scraping websites to get data for Dota analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a065cc",
   "metadata": {},
   "source": [
    "Importing selenium and the time module to control the browser, and make it wait due to some websites loading dynamically with AJAX, rendering some elements on the page invisible for a few seconds. bs4 to parse the HTML, and pandas to load data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d92ea356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "dc47c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_obj = Service('C:\\Program Files (x86)\\chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4af386",
   "metadata": {},
   "source": [
    "List of Dota player regions. Each region has it's own leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "881295b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = ['americas', 'europe', 'se_asia', 'china']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b7348",
   "metadata": {},
   "source": [
    "Function will use the provided argument to construct a URL and go to that webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5b839f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(region):\n",
    "    # Go to Dota Leaderboards page\n",
    "    driver.get(f'https://www.dota2.com/leaderboards/#{region}-0')\n",
    "    \n",
    "    # Give the page a second to load everything\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Grab the page source and put it into a variable\n",
    "    webpage = driver.page_source\n",
    "    \n",
    "    # Parse the page source as a soup object\n",
    "    soup = BeautifulSoup(webpage, 'html.parser')\n",
    "    \n",
    "    # Find the table tag\n",
    "    table = soup.tbody\n",
    "    \n",
    "    # Find all rows in table and put them into a list\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # Initialize lists to capture data\n",
    "    rankings = []\n",
    "    players = []\n",
    "    country_codes = []\n",
    "    \n",
    "    # Append each row's rank, and player name into rankings, and players respectively\n",
    "    for row in rows:\n",
    "        rank_tag = row.td\n",
    "        rankings.append(rank_tag.string)\n",
    "        player_tag = rank_tag.next_sibling\n",
    "        players.append(player_tag.get_text().strip())\n",
    "        # To account for any players that don't have a country listed\n",
    "        try:\n",
    "            flag = row.img['src']\n",
    "            country_code = flag[-6:-4]\n",
    "            country_codes.append(country_code.upper())\n",
    "        # If there's no country, replace missing value with an empty string\n",
    "        except:\n",
    "            country_codes.append('')\n",
    "            \n",
    "    # Put the lists into a dict and load as a dataframe\n",
    "    df = pd.DataFrame({'rank': rankings, 'player': players, 'country_code': country_codes})\n",
    "    \n",
    "    # Save dataframe to a CSV file, and remove default indexing\n",
    "    df.to_csv(f'Dota leaderboards {region.title()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e787fdc",
   "metadata": {},
   "source": [
    "Call the function over each element in the list of regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "db2f0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in all_regions:\n",
    "    scrape(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe1d0e",
   "metadata": {},
   "source": [
    "# Now scraping one more site to get country codes with their names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf77065",
   "metadata": {},
   "source": [
    "Using the requests module this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b6944cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80789a",
   "metadata": {},
   "source": [
    "Fetch the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6746808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.iban.com/country-codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7f163",
   "metadata": {},
   "source": [
    "Convert page into a soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6fb059e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c13e8",
   "metadata": {},
   "source": [
    "Find the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "675755bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('tbody')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62ceab",
   "metadata": {},
   "source": [
    "Find all the rows within the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c9895b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24de04",
   "metadata": {},
   "source": [
    "Initialize lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "00e0162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_names = []\n",
    "country_codes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0e451",
   "metadata": {},
   "source": [
    "Append each country's name, and 2 letter code into the lists respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9c78dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    name = row.td\n",
    "    country_names.append(name.string)\n",
    "    code = name.next_sibling.next_sibling\n",
    "    country_codes.append(code.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c0001",
   "metadata": {},
   "source": [
    "Store the lists into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f5aca8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'country': country_names, 'code': country_codes})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08433931",
   "metadata": {},
   "source": [
    "Save the dataframe without the indexing, to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ae466e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('country info.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
